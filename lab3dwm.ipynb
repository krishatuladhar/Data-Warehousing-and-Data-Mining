{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GkPkCpjoBJUYtMmg3AVqBteYpOHWjNRW",
      "authorship_tag": "ABX9TyP1+hBfjSDxjgPDX9ysK0N5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishatuladhar/Data-Warehousing-and-Data-Mining/blob/main/lab3dwm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lab 3\n",
        "* Write a program to implement ID3.\n",
        " (use the attached laptop_buy_data.csv)\n",
        "* Write a program to implement Naive Bayesian algorithm.\n",
        " (use the attached laptop_buy_data.csv)\n",
        "* Write a to implement classification by backpropagation on following data.\n",
        "\n",
        "       X1  X2   t\n",
        "      -1  -1  -1\n",
        "      -1   1   1\n",
        "       1  -1   1\n",
        "       1   1  -1\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wKbRGe5Ef2Mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qno 1:"
      ],
      "metadata": {
        "id": "HOuSGCoUgZoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Load data from a CSV file\n",
        "def load_data(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Calculate entropy of the target attribute\n",
        "def calculate_entropy(dataset, target):\n",
        "    class_counts = Counter(dataset[target])\n",
        "    entropy_value = 0.0\n",
        "    for count in class_counts.values():\n",
        "        probability = count / len(dataset)\n",
        "        entropy_value -= probability * math.log2(probability)\n",
        "    return entropy_value\n",
        "\n",
        "# Compute information gain of a specific attribute\n",
        "def compute_info_gain(dataset, attribute, target):\n",
        "    base_entropy = calculate_entropy(dataset, target)\n",
        "    values = dataset[attribute].unique()\n",
        "    weighted_entropy = 0.0\n",
        "\n",
        "    for value in values:\n",
        "        subset = dataset[dataset[attribute] == value]\n",
        "        weight = len(subset) / len(dataset)\n",
        "        weighted_entropy += weight * calculate_entropy(subset, target)\n",
        "\n",
        "    return base_entropy - weighted_entropy\n",
        "\n",
        "# Select the attribute with the highest information gain\n",
        "def select_best_attribute(dataset, attributes, target):\n",
        "    best_gain = float('-inf')\n",
        "    best_attribute = None\n",
        "\n",
        "    for attribute in attributes:\n",
        "        gain = compute_info_gain(dataset, attribute, target)\n",
        "        if gain > best_gain:\n",
        "            best_gain = gain\n",
        "            best_attribute = attribute\n",
        "\n",
        "    return best_attribute\n",
        "\n",
        "# Return the most frequent class value\n",
        "def get_majority_class(dataset, target):\n",
        "    return dataset[target].mode()[0]\n",
        "\n",
        "# ID3 algorithm implementation\n",
        "def build_decision_tree(dataset, attributes, target):\n",
        "    class_labels = dataset[target].unique()\n",
        "\n",
        "    # Case 1: all instances belong to the same class\n",
        "    if len(class_labels) == 1:\n",
        "        return class_labels[0]\n",
        "\n",
        "    # Case 2: no more attributes to split\n",
        "    if not attributes:\n",
        "        return get_majority_class(dataset, target)\n",
        "\n",
        "    # Choose the attribute with the highest info gain\n",
        "    best_attribute = select_best_attribute(dataset, attributes, target)\n",
        "    tree = {best_attribute: {}}\n",
        "\n",
        "    for value in dataset[best_attribute].unique():\n",
        "        subset = dataset[dataset[best_attribute] == value]\n",
        "        if subset.empty:\n",
        "            tree[best_attribute][value] = get_majority_class(dataset, target)\n",
        "        else:\n",
        "            remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n",
        "            subtree = build_decision_tree(subset, remaining_attributes, target)\n",
        "            tree[best_attribute][value] = subtree\n",
        "\n",
        "    return tree\n",
        "\n",
        "# Pretty-print the decision tree\n",
        "def display_tree(tree, indent=\"\"):\n",
        "    if not isinstance(tree, dict):\n",
        "        print(indent + \"➤ \" + str(tree))\n",
        "        return\n",
        "    for attribute, branches in tree.items():\n",
        "        for value, subtree in branches.items():\n",
        "            print(f\"{indent}[{attribute} = {value}]\")\n",
        "            display_tree(subtree, indent + \"  \")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    csv_path = \"/content/drive/MyDrive/datawarehousinglab/laptop_buy_data.csv\"\n",
        "    data = load_data(csv_path)\n",
        "\n",
        "    target_column = 'Class'  # Adjust if the target column has a different name\n",
        "    attribute_list = [column for column in data.columns if column != target_column]\n",
        "\n",
        "    decision_tree = build_decision_tree(data, attribute_list, target_column)\n",
        "    print(\"Generated Decision Tree:\")\n",
        "    display_tree(decision_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpkacl-Pf3fM",
        "outputId": "71aad731-952c-4a8a-ccc5-980b6bd1585f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Decision Tree:\n",
            "[Age = Youth]\n",
            "  [Student = Yes]\n",
            "    ➤ Buy\n",
            "  [Student = No]\n",
            "    [Credit_Rating = Excellent]\n",
            "      ➤ No\n",
            "    [Credit_Rating = Fair]\n",
            "      ➤ Buy\n",
            "[Age = Middle_Aged]\n",
            "  [Income = Low]\n",
            "    [Credit_Rating = Excellent]\n",
            "      ➤ No\n",
            "    [Credit_Rating = Fair]\n",
            "      [Student = Yes]\n",
            "        ➤ Buy\n",
            "  [Income = Medium]\n",
            "    ➤ No\n",
            "  [Income = High]\n",
            "    ➤ No\n",
            "[Age = Senior]\n",
            "  [Credit_Rating = Fair]\n",
            "    ➤ No\n",
            "  [Credit_Rating = Excellent]\n",
            "    [Income = Low]\n",
            "      ➤ Buy\n",
            "    [Income = High]\n",
            "      [Student = No]\n",
            "        ➤ Buy\n",
            "    [Income = Medium]\n",
            "      ➤ Buy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qno 2:"
      ],
      "metadata": {
        "id": "rdSN_SwTgeon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self):\n",
        "        self.class_probabilities = {}                 # Prior probabilities: P(Class)\n",
        "        self.conditional_probabilities = defaultdict(dict)  # Conditional: P(Attribute=val | Class)\n",
        "\n",
        "    def fit(self, data, target_attribute):\n",
        "        self.attributes = [col for col in data.columns if col != target_attribute]\n",
        "        total_records = len(data)\n",
        "\n",
        "        # Calculate prior probabilities for each class\n",
        "        class_counts = data[target_attribute].value_counts()\n",
        "        self.class_probabilities = {\n",
        "            label: count / total_records for label, count in class_counts.items()\n",
        "        }\n",
        "\n",
        "        # Compute conditional probabilities for each attribute value given class\n",
        "        for attribute in self.attributes:\n",
        "            unique_vals = data[attribute].unique()\n",
        "            for label in class_counts.index:\n",
        "                subset = data[data[target_attribute] == label]\n",
        "                value_counts = subset[attribute].value_counts()\n",
        "                total_in_class = len(subset)\n",
        "\n",
        "                for val in unique_vals:\n",
        "                    # Apply Laplace smoothing\n",
        "                    count = value_counts.get(val, 0)\n",
        "                    smoothed_prob = (count + 1) / (total_in_class + len(unique_vals))\n",
        "                    self.conditional_probabilities[(attribute, val)][label] = smoothed_prob\n",
        "\n",
        "    def predict(self, instance):\n",
        "        class_scores = {}\n",
        "\n",
        "        for label in self.class_probabilities:\n",
        "            probability = self.class_probabilities[label]\n",
        "\n",
        "            for attribute in self.attributes:\n",
        "                value = instance.get(attribute)\n",
        "                # Use a small fallback probability for unseen values\n",
        "                conditional = self.conditional_probabilities.get((attribute, value), {}).get(label, 1e-6)\n",
        "                probability *= conditional\n",
        "\n",
        "            class_scores[label] = probability\n",
        "\n",
        "        # Return the class with the highest posterior probability\n",
        "        return max(class_scores, key=class_scores.get)\n",
        "\n",
        "    def predict_all(self, dataframe):\n",
        "        return [self.predict(row) for _, row in dataframe.iterrows()]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load dataset\n",
        "    filepath = \"/content/drive/MyDrive/datawarehousinglab/laptop_buy_data.csv\"\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Train Naive Bayes model\n",
        "    classifier = NaiveBayesClassifier()\n",
        "    classifier.fit(df, target_attribute='Class')\n",
        "\n",
        "    # Example test instance\n",
        "    test_sample = {\n",
        "        'Age': 'Senior',\n",
        "        'Income': 'Medium',\n",
        "        'Student': 'No',\n",
        "        'Credit_Rating': 'Excellent'\n",
        "    }\n",
        "\n",
        "    result = classifier.predict(test_sample)\n",
        "    print(\"Prediction for test instance:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxOvuGcmggsu",
        "outputId": "a4c7d75c-b07f-4054-acca-48f4f475a4c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for test instance: No\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qno 3:"
      ],
      "metadata": {
        "id": "zbOAxjxUgrZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Input data (4 samples, 2 features)\n",
        "X = np.array([\n",
        "    [-1, -1],\n",
        "    [-1,  1],\n",
        "    [ 1, -1],\n",
        "    [ 1,  1]\n",
        "])\n",
        "\n",
        "# Target values reshaped to column vector\n",
        "y = np.array([[-1], [1], [1], [-1]])\n",
        "\n",
        "# Scale y to range [0, 1] to match sigmoid output\n",
        "y_scaled = (y + 1) / 2\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(1)\n",
        "\n",
        "# Initialize weights and biases\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "# Weights\n",
        "W1 = 2 * np.random.rand(input_size, hidden_size) - 1\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "W2 = 2 * np.random.rand(hidden_size, output_size) - 1\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # --- Forward Pass ---\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # --- Backward Pass ---\n",
        "    error = y_scaled - a2\n",
        "    d_output = error * sigmoid_derivative(a2)\n",
        "\n",
        "    d_hidden = d_output.dot(W2.T) * sigmoid_derivative(a1)\n",
        "\n",
        "    # --- Weight Update ---\n",
        "    W2 += a1.T.dot(d_output) * learning_rate\n",
        "    b2 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    W1 += X.T.dot(d_hidden) * learning_rate\n",
        "    b1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "# Final prediction\n",
        "output = sigmoid(np.dot(sigmoid(np.dot(X, W1) + b1), W2) + b2)\n",
        "predicted = (output > 0.5).astype(int)\n",
        "true_label = (y_scaled > 0.5).astype(int)\n",
        "\n",
        "print(\"Predicted outputs (0 or 1):\\n\", predicted)\n",
        "print(\"Actual outputs:\\n\", true_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NzLrEykgu-p",
        "outputId": "1d1e45a7-28a4-4bdc-a4a0-10aa5e543346"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted outputs (0 or 1):\n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "Actual outputs:\n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ]
    }
  ]
}