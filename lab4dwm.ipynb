{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HwiQa4-7tBCXw6qXAj52_8f6gI4y-64s",
      "authorship_tag": "ABX9TyMrvcJs0geJXFaSZL9ShM8j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishatuladhar/Data-Warehousing-and-Data-Mining/blob/main/lab4dwm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Write a program to implement the k-means algorithm.\n",
        "* Write a program to implement the k-means ++ algorithm.\n",
        "* Write a program to implement k-medoids algorithm.\n",
        "* Write a program to implement an agglomerative hierarchical  clustering algorithm.\n",
        "* Write a program to implement a divisive algorithm.\n",
        "* Write a program to implement the DBSCAN algorithm.\n",
        "* Write a program to perform outlier analysis using the Z-score method.\n",
        "* Write a program to perform outlier detection using IQR method.\n",
        "* Write a program to implement a mini-batch k-means algorithm.\n",
        "\n",
        "\n",
        "Note:\n",
        "You are expected to create a custom function that implements the specified algorithm..\n",
        "Use “data.csv” for 1-5.\n",
        "Use “data2.csv” for 6.\n",
        "Use “data3.csv” for outlier analysis.\n",
        "Use “data4.csv” for 9.\n"
      ],
      "metadata": {
        "id": "zRmH7jcAjY5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRWaR5olh6Ug",
        "outputId": "3f4ec032-f5b3-4bb0-8ebe-3e0d21dc1913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Centroids: [[1.         6.        ]\n",
            " [1.90763796 1.92106347]\n",
            " [5.98432967 5.31352018]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def euclidean(a, b):\n",
        "    return np.sqrt(np.sum((a - b) ** 2))\n",
        "\n",
        "def kmeans(data, k, max_iter=100):\n",
        "    n_samples, n_features = data.shape\n",
        "    centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        labels = np.array([np.argmin([euclidean(x, c) for c in centroids]) for x in data])\n",
        "        new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "        if np.all(centroids == new_centroids):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return centroids, labels\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data.csv').values\n",
        "centroids, labels = kmeans(data, k=3)\n",
        "print(\"Centroids:\", centroids)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def kmeans_pp_init(data, k):\n",
        "    n_samples = data.shape[0]\n",
        "    centroids = [data[np.random.randint(n_samples)]]\n",
        "\n",
        "    for _ in range(1, k):\n",
        "        distances = np.array([min([np.linalg.norm(x - c) for c in centroids])**2 for x in data])\n",
        "        probs = distances / distances.sum()\n",
        "        cumulative_probs = np.cumsum(probs)\n",
        "        r = np.random.rand()\n",
        "        for i, p in enumerate(cumulative_probs):\n",
        "            if r < p:\n",
        "                centroids.append(data[i])\n",
        "                break\n",
        "    return np.array(centroids)\n",
        "\n",
        "def kmeans_pp(data, k, max_iter=100):\n",
        "    centroids = kmeans_pp_init(data, k)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        labels = np.array([np.argmin([np.linalg.norm(x - c) for c in centroids]) for x in data])\n",
        "        new_centroids = np.array([data[labels == i].mean(axis=0) for i in range(k)])\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return centroids, labels\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data.csv').values\n",
        "centroids, labels = kmeans_pp(data, k=3)\n",
        "print(\"Centroids:\", centroids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpusigjdioFh",
        "outputId": "099c2cb4-b313-4c47-b246-5ba180dba8f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Centroids: [[1.82512542 2.29187589]\n",
            " [5.88276264 5.7448722 ]\n",
            " [7.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def k_medoids(data, k, max_iter=100):\n",
        "    m, n = data.shape\n",
        "    medoids_idx = np.random.choice(m, k, replace=False)\n",
        "    medoids = data[medoids_idx]\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        labels = np.argmin([[np.sum(np.abs(x - m)) for m in medoids] for x in data], axis=1)\n",
        "\n",
        "        new_medoids = []\n",
        "        for i in range(k):\n",
        "            cluster = data[labels == i]\n",
        "            costs = [np.sum(np.abs(point - cluster).sum(axis=1)) for point in cluster]\n",
        "            new_medoids.append(cluster[np.argmin(costs)])\n",
        "\n",
        "        new_medoids = np.array(new_medoids)\n",
        "        if np.allclose(medoids, new_medoids):\n",
        "            break\n",
        "        medoids = new_medoids\n",
        "\n",
        "    return medoids, labels\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data.csv').values\n",
        "medoids, labels = k_medoids(data, k=3)\n",
        "print(\"Medoids:\", medoids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM2hKyfXir7J",
        "outputId": "eee75017-c4b9-4824-df18-79783d66b3ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Medoids: [[2.24835708 1.93086785]\n",
            " [1.49358444 2.15712367]\n",
            " [5.57955292 5.79581438]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "def agglomerative_clustering(data, k):\n",
        "    clusters = [[i] for i in range(len(data))]\n",
        "    distances = squareform(pdist(data))\n",
        "\n",
        "    np.fill_diagonal(distances, np.inf)\n",
        "\n",
        "    while len(clusters) > k:\n",
        "        i, j = np.unravel_index(np.argmin(distances), distances.shape)\n",
        "        new_cluster = clusters[i] + clusters[j]\n",
        "        clusters.append(new_cluster)\n",
        "\n",
        "        # Update distances\n",
        "        new_dist = np.minimum(distances[i], distances[j])\n",
        "        distances = np.vstack([distances, new_dist])\n",
        "        new_dist = np.append(new_dist, np.inf)\n",
        "        distances = np.column_stack([distances, new_dist])\n",
        "\n",
        "        # Remove merged clusters\n",
        "        distances = np.delete(distances, [i, j], axis=0)\n",
        "        distances = np.delete(distances, [i, j], axis=1)\n",
        "        clusters.pop(max(i, j))\n",
        "        clusters.pop(min(i, j))\n",
        "\n",
        "    return clusters\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data.csv').values\n",
        "clusters = agglomerative_clustering(data, k=3)\n",
        "print(\"Clusters:\", clusters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYGUg8lKiw3R",
        "outputId": "84246cc1-ede3-4fea-b344-a9a2097679cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clusters: [[21], [15, 10, 13, 19, 12, 14, 17, 18, 11, 16], [20, 1, 3, 6, 7, 9, 4, 8, 0, 2, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def divisive(data, k):\n",
        "    clusters = [list(range(len(data)))]\n",
        "\n",
        "    while len(clusters) < k:\n",
        "        # Find cluster with largest diameter\n",
        "        max_diameter, split_idx = -1, -1\n",
        "        for idx, cluster in enumerate(clusters):\n",
        "            points = data[cluster]\n",
        "            diam = np.max(pdist(points)) if len(points) > 1 else 0\n",
        "            if diam > max_diameter:\n",
        "                max_diameter = diam\n",
        "                split_idx = idx\n",
        "\n",
        "        to_split = clusters.pop(split_idx)\n",
        "        points = data[to_split]\n",
        "        # Use simple 2-means to split\n",
        "        c1, c2 = np.mean(points, axis=0), points[np.argmax(np.linalg.norm(points - np.mean(points, axis=0), axis=1))]\n",
        "        labels = [0 if np.linalg.norm(p - c1) < np.linalg.norm(p - c2) else 1 for p in points]\n",
        "\n",
        "        cluster1 = [to_split[i] for i in range(len(to_split)) if labels[i]==0]\n",
        "        cluster2 = [to_split[i] for i in range(len(to_split)) if labels[i]==1]\n",
        "\n",
        "        clusters.extend([cluster1, cluster2])\n",
        "\n",
        "    return clusters\n",
        "\n",
        "from scipy.spatial.distance import pdist\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data.csv').values\n",
        "clusters = divisive(data, k=3)\n",
        "print(\"Clusters:\", clusters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VE9W4kai23a",
        "outputId": "95077ee6-66aa-40e2-cb09-835a24bd3659"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clusters: [[21], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 18, 20], [10, 11, 12, 13, 14, 15, 16, 17, 19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def dbscan(data, eps, min_pts):\n",
        "    labels = np.full(len(data), -1)\n",
        "    cluster_id = 0\n",
        "\n",
        "    def region_query(p):\n",
        "        return [i for i, q in enumerate(data) if np.linalg.norm(p - q) < eps]\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        if labels[i] != -1:\n",
        "            continue\n",
        "        neighbors = region_query(data[i])\n",
        "\n",
        "        if len(neighbors) < min_pts:\n",
        "            labels[i] = -1  # Noise\n",
        "        else:\n",
        "            labels[i] = cluster_id\n",
        "            seeds = neighbors.copy()\n",
        "            seeds.remove(i)\n",
        "            while seeds:\n",
        "                j = seeds.pop()\n",
        "                if labels[j] == -1:\n",
        "                    labels[j] = cluster_id\n",
        "                if labels[j] != -1:\n",
        "                    continue\n",
        "                labels[j] = cluster_id\n",
        "                new_neighbors = region_query(data[j])\n",
        "                if len(new_neighbors) >= min_pts:\n",
        "                    seeds.extend(new_neighbors)\n",
        "            cluster_id += 1\n",
        "    return labels\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data2.csv').values\n",
        "labels = dbscan(data, eps=0.5, min_pts=5)\n",
        "print(\"Cluster Labels:\", labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OSxGBNEi_a0",
        "outputId": "12ffdf9a-be79-4489-a765-48d29f732a60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Labels: [ 0  1  1  2  3  3  4  2  5  6  4  5  2  0  5  5  6  2  4  4  3  1  2  0\n",
            "  4  4  4  4  3  3  1  1  6  4  2  7  3  3  2  4  4  8  6  1  3  8  1  1\n",
            "  2  0  3  5  1  3  3  4  3  0  5  0  5  0  6  1  6  3  6  5  0  3  6  3\n",
            "  2  6  3  3  1  9  6  3  1  4  6  5  5  0  2  6  5  4  1  3  7  6  1  8\n",
            "  2  1  6  3  1 10  3  2  3  2  1  4  3  2  2  2  8  2  4  3  4  6  2  1\n",
            "  6  3  2  2  1  1  5  2  0  6  2  6  0  2  6  3  1  7  6  3  5  5  5  6\n",
            "  0  6  1  3  0  0  5  5  0  1  1  4  5  3  1  4  5  3  2  5  4  4  7  6\n",
            "  5  4  6  0  1  4  2  5  3  6  2  5  2  6  2  3  2  6  0  3  0  4  3  8\n",
            "  1  5  4  7  1  0  5  7 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def z_score_outliers(data, threshold=3):\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    z_scores = (data - mean) / std\n",
        "    return np.where(np.abs(z_scores) > threshold)\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data3.csv').values.flatten()\n",
        "outliers = z_score_outliers(data)\n",
        "print(\"Outliers at indices:\", outliers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMq8Cd7yjEeR",
        "outputId": "f6daee23-6148-42bf-8be0-527934d88e17"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers at indices: (array([], dtype=int64),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def iqr_outliers(data):\n",
        "    q1, q3 = np.percentile(data, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
        "    return np.where((data < lower) | (data > upper))\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data3.csv').values.flatten()\n",
        "outliers = iqr_outliers(data)\n",
        "print(\"Outliers at indices:\", outliers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fBH0XuOjIho",
        "outputId": "9a2fc243-f37a-4b30-b459-a1deb4b650c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers at indices: (array([ 6, 14]),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def mini_batch_kmeans(data, k, batch_size=10, max_iter=100):\n",
        "    n_samples, n_features = data.shape\n",
        "    centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        batch_idx = np.random.choice(n_samples, batch_size, replace=False)\n",
        "        batch = data[batch_idx]\n",
        "        labels = np.array([np.argmin([np.linalg.norm(x - c) for c in centroids]) for x in batch])\n",
        "\n",
        "        for i in range(k):\n",
        "            points = batch[labels == i]\n",
        "            if len(points):\n",
        "                centroids[i] = centroids[i] + 0.1 * (points.mean(axis=0) - centroids[i])  # update rule\n",
        "\n",
        "    return centroids\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/datawarehousinglab/data4.csv').values\n",
        "centroids = mini_batch_kmeans(data, k=3)\n",
        "print(\"Centroids:\", centroids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx-Md_jDjNKD",
        "outputId": "a380a7b9-6304-43e6-833c-13042cbb8401"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Centroids: [[-6.92138485 -6.86424713]\n",
            " [ 4.62294348  2.08871658]\n",
            " [-2.49590217  9.03427446]]\n"
          ]
        }
      ]
    }
  ]
}