{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "131g2tnJJCGT5DTWw8In7ojvO7L_jB4BA",
      "authorship_tag": "ABX9TyN/cWSFpjbxkeGvvmcAvMwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishatuladhar/Data-Warehousing-and-Data-Mining/blob/main/lab2dwm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lab2\n",
        "#Apriori algorithm:\n",
        "* Find frequently occurring itemsets using the Apriori algorithm.\n",
        "* Compute the support of the frequent itemset.\n",
        "* Compute the confidence and lift of an association rule.\n",
        "\n",
        "#FP-Growth algorithm.\n",
        "* Find frequently occurring itemsets using the FP-Growth algorithm.\n",
        "* Compute the support of the frequent itemset.\n",
        "* Compute the confidence and lift of an association rule.\n",
        "* Compare Apriori and FP-growth algorithms.\n",
        "* Note: Use the sports.txt and space.txt as input data.\n"
      ],
      "metadata": {
        "id": "w2l1jl8SAqe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequent Itemset Mining using Apriori and FP-Growth\n",
        "\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
        "import time\n",
        "\n",
        "# Settings\n",
        "MIN_SUPPORT = 0.15\n",
        "MIN_CONFIDENCE = 0.7\n",
        "FILE_PATHS = {\n",
        "    'space': '/content/drive/MyDrive/datawarehousinglab/space.txt',\n",
        "    'sports': '/content/drive/MyDrive/datawarehousinglab/sports.txt'\n",
        "}\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def get_support(itemset, transactions):\n",
        "    return sum(1 for tx in transactions if itemset.issubset(set(tx))) / len(transactions)\n",
        "\n",
        "def apriori_algorithm(transactions, min_support):\n",
        "    total = len(transactions)\n",
        "    item_counts = defaultdict(int)\n",
        "\n",
        "    # Step 1: Count individual items\n",
        "    for tx in transactions:\n",
        "        for item in tx:\n",
        "            item_counts[frozenset([item])] += 1\n",
        "\n",
        "    freq_items = {item: count for item, count in item_counts.items() if count / total >= min_support}\n",
        "    all_frequent = freq_items.copy()\n",
        "    current_freq = list(freq_items.keys())\n",
        "    k = 2\n",
        "\n",
        "    # Step 2: Generate combinations\n",
        "    while current_freq:\n",
        "        candidates = set()\n",
        "        for i in range(len(current_freq)):\n",
        "            for j in range(i + 1, len(current_freq)):\n",
        "                union = current_freq[i] | current_freq[j]\n",
        "                if len(union) == k:\n",
        "                    candidates.add(union)\n",
        "\n",
        "        candidate_counts = defaultdict(int)\n",
        "        for tx in transactions:\n",
        "            tx_set = set(tx)\n",
        "            for candidate in candidates:\n",
        "                if candidate.issubset(tx_set):\n",
        "                    candidate_counts[candidate] += 1\n",
        "\n",
        "        current_freq = [item for item in candidate_counts if candidate_counts[item] / total >= min_support]\n",
        "        all_frequent.update({item: candidate_counts[item] for item in current_freq})\n",
        "        k += 1\n",
        "\n",
        "    return all_frequent\n",
        "\n",
        "def extract_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    rules = []\n",
        "    total = len(transactions)\n",
        "\n",
        "    for itemset in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        support_itemset = frequent_itemsets[itemset] / total\n",
        "\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in combinations(itemset, i):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                support_ante = get_support(antecedent, transactions)\n",
        "                support_cons = get_support(consequent, transactions)\n",
        "                confidence = support_itemset / support_ante\n",
        "                lift = confidence / support_cons\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append({\n",
        "                        'antecedents': set(antecedent),\n",
        "                        'consequents': set(consequent),\n",
        "                        'support': round(support_itemset, 2),\n",
        "                        'confidence': round(confidence, 2),\n",
        "                        'lift': round(lift, 2)\n",
        "                    })\n",
        "    return rules\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def analyze_dataset(name, path):\n",
        "    print(f\"\\n=== Analyzing '{name}' Dataset ===\")\n",
        "\n",
        "    # Load transaction data\n",
        "    with open(path, 'r') as file:\n",
        "        next(file)\n",
        "        transactions = [[item.strip() for item in line.strip().split(',')[1:] if item.strip()] for line in file]\n",
        "\n",
        "    total = len(transactions)\n",
        "\n",
        "    # --- Apriori ---\n",
        "    print(\"\\n[Apriori Algorithm]\")\n",
        "    start_apriori = time.time()\n",
        "    apriori_itemsets = apriori_algorithm(transactions, MIN_SUPPORT)\n",
        "    apriori_rules = extract_rules(apriori_itemsets, transactions, MIN_CONFIDENCE)\n",
        "    end_apriori = time.time()\n",
        "\n",
        "    apriori_df = pd.DataFrame([{\n",
        "        'itemsets': set(item),\n",
        "        'support': round(count / total, 2)\n",
        "    } for item, count in apriori_itemsets.items()])\n",
        "\n",
        "    if not apriori_rules:\n",
        "        print(\"No association rules found with confidence >=\", MIN_CONFIDENCE)\n",
        "    else:\n",
        "        apriori_rules_df = pd.DataFrame(apriori_rules)\n",
        "        print(apriori_rules_df[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "\n",
        "    # --- FP-Growth ---\n",
        "    print(\"\\n[FP-Growth Algorithm]\")\n",
        "    start_fp = time.time()\n",
        "    te = TransactionEncoder()\n",
        "    df_encoded = pd.DataFrame(te.fit_transform(transactions), columns=te.columns_)\n",
        "    fp_itemsets = fpgrowth(df_encoded, min_support=MIN_SUPPORT, use_colnames=True)\n",
        "    fp_rules = association_rules(fp_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
        "    end_fp = time.time()\n",
        "\n",
        "    if fp_rules.empty:\n",
        "        print(\"No association rules found using FP-Growth with confidence >=\", MIN_CONFIDENCE)\n",
        "    else:\n",
        "        print(fp_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
        "\n",
        "    # --- Time Comparison ---\n",
        "    print(\"\\n[Performance]\")\n",
        "    print(f\"Apriori Time: {round(end_apriori - start_apriori, 4)}s\")\n",
        "    print(f\"FP-Growth Time: {round(end_fp - start_fp, 4)}s\")\n",
        "\n",
        "    print(\"\\n[Summary]\")\n",
        "    print(f\"Apriori Rules: {len(apriori_rules)}\")\n",
        "    print(f\"FP-Growth Rules: {len(fp_rules)}\")\n",
        "    print(\"FP-Growth is faster.\" if end_fp < end_apriori else \"Apriori is faster.\")\n",
        "\n",
        "# --- Run on All Datasets ---\n",
        "for name, path in FILE_PATHS.items():\n",
        "    analyze_dataset(name, path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08iTktkJ5zIJ",
        "outputId": "c00ff73c-e9b1-4149-a819-15e680b93c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Analyzing 'space' Dataset ===\n",
            "\n",
            "[Apriori Algorithm]\n",
            "No association rules found with confidence >= 0.7\n",
            "\n",
            "[FP-Growth Algorithm]\n",
            "No association rules found using FP-Growth with confidence >= 0.7\n",
            "\n",
            "[Performance]\n",
            "Apriori Time: 0.0004s\n",
            "FP-Growth Time: 0.0431s\n",
            "\n",
            "[Summary]\n",
            "Apriori Rules: 0\n",
            "FP-Growth Rules: 0\n",
            "Apriori is faster.\n",
            "\n",
            "=== Analyzing 'sports' Dataset ===\n",
            "\n",
            "[Apriori Algorithm]\n",
            "No association rules found with confidence >= 0.7\n",
            "\n",
            "[FP-Growth Algorithm]\n",
            "No association rules found using FP-Growth with confidence >= 0.7\n",
            "\n",
            "[Performance]\n",
            "Apriori Time: 0.0004s\n",
            "FP-Growth Time: 0.0231s\n",
            "\n",
            "[Summary]\n",
            "Apriori Rules: 0\n",
            "FP-Growth Rules: 0\n",
            "Apriori is faster.\n"
          ]
        }
      ]
    }
  ]
}